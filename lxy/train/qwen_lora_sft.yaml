### model
model_name_or_path: /home/disk-380/models/Qwen/Qwen2.5-7B
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
dataset: huatuo_26m_lite
dataset_dir: /home/disk-500/code/LLaMA-Factory/lxy/data
template: qwen
cutoff_len: 512
max_samples: 30000
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: /home/disk-500/code/LLaMA-Factory/lxy/saves
logging_steps: 20
save_steps: 100
plot_loss: true
overwrite_output_dir: true

### train    
per_device_train_batch_size: 4
gradient_accumulation_steps: 32
learning_rate: 5.0e-6
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
# warmup_steps: 20
fp16: true # 还有bp16, pure_bf16可用
ddp_timeout: 180000000
load_best_model_at_end: true
# report_to: tensorboard
# logging_dir: /home/disk-500/code/LLaMA-Factory/lxy/saves/tensonboard/runs

### eval
val_size: 0.1
per_device_eval_batch_size: 8
eval_steps: 50
eval_strategy: steps